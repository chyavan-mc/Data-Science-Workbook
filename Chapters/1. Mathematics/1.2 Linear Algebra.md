# Linear Algebra

Linear Algebra is concerned with two **Fundamental Operations** on vectors
* Vector Addition
* Scalar Multiplication


## Basic Definitions

1. **Scalar**: A scalar is a *single* numerical quantity, represented by a single real or complex number.
2. **Vector**: A vector is a *1 dimensional ordered set of numbers* that has both magnitude and direction. *Note: Dimensionality of a vector is the number of elements in the 1-D set that represents the vector*.
3. **Matrix**: A matrix is a *2 dimensional array of numbers* arranged in rows and columns. Matrices are used to represent data or perform various operations in linear algebra.
4. **Tensor**: A tensor is a generalization of scalars, vectors, and matrices that can represent multidimensional data. Tensors can have an arbitrary number of dimensions and components, and they obey specific transformation rules under coordinate changes.


## More Basic Definitions

1. **Span**: The span of a set of vectors is defined as the set of all linear combinations of the vectors.


$$Span(\vec{w_1},\vec{w_2},...,\vec{w_n}) = \{C_1\vec{w_1} + C_2\vec{w_2} + ... + C_n\vec{w_n} : C_i \in \mathbb{R} \:\:\: \forall \: i \leq n \}$$


2. **Linear Dependence**
    * **Linearly Independent Vectors**: A set of vectors ($\vec{v_1}, \vec{v_2},...\vec{v_n}$) are said to be *Linearly Independent*, if $C_1\vec{v_1} + C_2\vec{v_2} + ... + C_n\vec{v_n} = 0$ only when all the scalars are zero ($C_1=C_2=...=C_n=0$)
    * **Linearly Dependent Vectors**: A set of vectors ($\vec{v_1}, \vec{v_2},...\vec{v_n}$) are said to be *Linearly Dependent*, if one of the vectors can be expressed as a linear combination of the other vectors.
        * If one of the vectors in the set is $\vec{0}$, then the set is linearly dependent
        * A set of $n$ vectors in $\mathbb{R}^m$ are linearly dependent if $n > m$
        * The columns of a matrix $\mathbf{A}$ are linearly independent if $N(A) = \{0\}$
3. **Basis** - The basis of a vector space is a set of *<u>linearly independent</u>* vectors that *<u>span</u>* the full vector space.


## Matrices and Transformations

Matrices are used to describe linear transformations where each column represents the location of basis vectors of the vector space after the transformation. Matrix vector multiplications is therefore a way to apply a transformation to a vector


### Linear Transformations

A trasformation is a **Linear transformation** if 
* *Gridlines remain parallel and evenly spaced
* The origin should be fixed

Mathematically, a transformation T is a **Linear transformation** if
* $T(\vec{a}+\vec{b}) = T(\vec{a}) + T(\vec{b})$
* $T(c\vec{a}) = cT(\vec{a})$

*<u>Note</u>: All matrix transformations are linear*


### Composition of Transformations

Composition of transformations refers to the operation of applying one matrix transformation followed by another. For a vector $\vec{v}$, if we first apply a transformation $\mathbf{A}$ and then applying $\mathbf{B}$. The composition of transformations can be defined by another single transformation $\mathbf{C}$, where $\mathbf{C} = \mathbf{B} \mathbf{A}$


Matrix multiplication is *generally* not commutative. That is, $\mathbf{A}\mathbf{B} \neq \mathbf{B}\mathbf{A}$.
* $\begin{bmatrix} 1 & 2 \\ -1 & 0 \end{bmatrix} \begin{bmatrix} -3 & 0 \\ 0 & -3 \end{bmatrix} = \begin{bmatrix} -3 & 0 \\ 0 & -3 \end{bmatrix} \begin{bmatrix} 1 & 2 \\ -1 & 0 \end{bmatrix} = \begin{bmatrix} -3 & -6 \\ 3 & 0  \end{bmatrix}$
* In the above example, one of the transformations is just a scaling transformation (or an identity transformation hiding behind a scalar) and hence the multiplication is commutative.


### Determinant of a Matrix

The scalar by which the transformation that the matrix represents scales a unit volume (n-dimensional volume) when the transformation is applied. The sign of the determinant indicates orientation of the basis vectors after the transformation.


